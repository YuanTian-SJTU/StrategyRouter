import json
import multiprocessing
from typing import Collection, Any
from matplotlib import pyplot as plt
import http.client

from implementation import funsearch
from implementation import config
from implementation import sampler
from implementation import evaluator_accelerate
from implementation import evaluator
from implementation import code_manipulation
from implementation import strategy_tracker
import bin_packing_utils

import json
import multiprocessing
from typing import Collection, Any
import http.client
import numpy as np
import time

# 读取API密钥
with open('api_key.txt', 'r') as f:
    api_key = f.read()

# 记录分数
scores_list = []
# 记录不同策略的分数
strategy_scores = {
    "Hybrid": [],
    "First Fit": [],
    "Best Fit": [],
    "Worst Fit": [],
    "Greedy": [],
    "Other": []
}

# 使用给定策略的概率
pb = 0.15
# 使用给定策略的次数
fixed_count = {
    'First Fit': 0,
    'Best Fit': 0,
    'Worst Fit': 0,
    'Greedy': 0
}
# 错误函数数量
failed_count = 0

def _trim_preface_of_body(sample: str) -> str:
    """Trim the redundant descriptions/symbols/'def' declaration before the function body.
    Please see my comments in sampler.LLM (in sampler.py).
    Since the LLM used in this file is not a pure code completion LLM, this trim function is required.

    -Example sample (function & description generated by LLM):
    -------------------------------------
    This is the optimized function ...
    def priority_v2(...) -> ...:
        return ...
    This function aims to ...
    -------------------------------------
    -This function removes the description above the function's signature, and the function's signature.
    -The indent of the code is preserved.
    -Return of this function:
    -------------------------------------
        return ...
    This function aims to ...
    -------------------------------------
    """
    lines = sample.splitlines()
    func_body_lineno = 0
    find_def_declaration = False
    for lineno, line in enumerate(lines):
        # find the first 'def' statement in the given code
        if line[:3] == 'def':
            func_body_lineno = lineno
            find_def_declaration = True
            break
    if find_def_declaration:
        code = ''
        for line in lines[func_body_lineno + 1:]:
            code += line + '\n'
        return code
    return sample


class LLMAPI(sampler.LLM):
    """Language model that predicts continuation of provided source code.
    """

    def __init__(self, samples_per_prompt: int, trim=True):
        super().__init__(samples_per_prompt)
        self._trim = trim

    def draw_samples(self, prompt: str) -> Collection[str]:
        """Returns multiple predicted continuations of `prompt`."""
        return [self._draw_sample(prompt) for _ in range(self._samples_per_prompt)]

    def _draw_sample(self, content: str) -> str:
        # 提取策略分
        global strategy_scores
        strategy_prompt = ''
        for strategy, scores in strategy_scores.items():
            if scores:
                score = max(scores)
                strategy_prompt += f"{strategy}: Best score {score:.2f}\n"
            else:
                strategy_prompt += f"{strategy}: Unknown\n"
        # 随机触发使用固定策略生成函数
        if np.random.random() <= pb:
            strategy = np.random.choice(['First Fit', 'Best Fit', 'Worst Fit', 'Greedy'])
            global fixed_count
            fixed_count[strategy] += 1
        else:
            strategy = None

        if strategy is not None:
            additional_prompt = (
                'Complete a different and more complex Python function. '
                f'You are strongly recommended to use {strategy} strategy. '
                'Only output the Python code, no descriptions.'
                'In the function docstring, clearly state which strategy you are using.'
            )
        else:
            additional_prompt = (
                'Complete a different and more complex Python function. '
                'Be creative and you can implement various strategies like First Fit, Best Fit, Worst Fit, or Greedy approaches. '
                'You can also combine multiple strategies or create new ones. '
                'Only output the Python code, no descriptions.'
                'In the function docstring, clearly state which strategy you are using.'
                f'Current strategy scores:\n {strategy_prompt}'
            )
        prompt = '\n'.join([content, additional_prompt])

        # print(prompt)
        
        while True:
            # 连接API信息
            try:
                conn = http.client.HTTPSConnection("api.siliconflow.cn")
                payload = json.dumps({
                    "max_tokens": 512,
                    "model": "THUDM/GLM-4-32B-0414",
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ]
                })
                headers = {
                    'Authorization': 'Bearer {}'.format(api_key),
                    'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',
                    'Content-Type': 'application/json'
                }
                conn.request("POST", "/v1/chat/completions", payload, headers)
                res = conn.getresponse()
                data = res.read().decode("utf-8")
                data = json.loads(data)
                response = data['choices'][0]['message']['content']
                # trim function
                if self._trim:
                    response = _trim_preface_of_body(response)
                return response
            except Exception:
                continue


class Sandbox(evaluator.Sandbox):
    """Sandbox for executing generated code. Implemented by RZ.

    RZ: Sandbox returns the 'score' of the program and:
    1) avoids the generated code to be harmful (accessing the internet, take up too much RAM).
    2) stops the execution of the code in time (avoid endless loop).
    """

    def __init__(self, verbose=False, numba_accelerate=True):
        """
        Args:
            verbose         : Print evaluate information.
            numba_accelerate: Use numba to accelerate the evmport abstractmethod, ABCaluation. It should be noted that not all numpy functions
                              support numba acceleration, such as np.piecewise().
        """
        self._verbose = verbose
        self._numba_accelerate = numba_accelerate
        self._strategy_tracker = strategy_tracker.StrategyTracker()
    
    def run(
            self,
            program: str,
            function_to_run: str,
            function_to_evolve: str,
            inputs: Any,
            test_input: str,
            timeout_seconds: int,
            **kwargs
    ) -> tuple[Any, bool]:
        """Returns `function_to_run(test_input)` and whether execution succeeded."""
        dataset = inputs[test_input]
        result_queue = multiprocessing.Queue()
        process = multiprocessing.Process(
            target=self._compile_and_run_function,
            args=(program, function_to_run, function_to_evolve, dataset, self._numba_accelerate, result_queue)
        )
        process.start()
        process.join(timeout=timeout_seconds)
        if process.is_alive():
            # if the process is not finished in time, we consider the program illegal
            process.terminate()
            process.join()
            results = None, False
        else:
            if not result_queue.empty():
                results = result_queue.get_nowait()
            else:
                results = None, False

        if self._verbose:
            print(f'================= Evaluated Program =================')
            program_: code_manipulation.Program = code_manipulation.text_to_program(text=program)
            func_to_evolve_: str = kwargs.get('func_to_evolve', 'priority')
            function_: code_manipulation.Function = program_.get_function(func_to_evolve_)
            function_: str = str(function_).strip('\n')
            print(f'{function_}')
            print(f'-----------------------------------------------------')
            print(f'Score: {str(results)}')
            print(f'=====================================================')
            print(f'\n')
        
        # 记录当前最高分和策略分数
        global strategy_scores  # 显式声明为全局变量
        if results[0] is not None:  # 如果分数不为空
            # 分数列表
            if not scores_list: # 如果分数列表为空，直接添加当前分数
                scores_list.append(results[0])
            else:   # 如果分数列表不为空，添加当前分数和列表中上一分数更高的一个
                scores_list.append(max(scores_list[-1], results[0]))

            # 策略分数
            strategy_scores = self._strategy_tracker.update_score(program, results[0])
        else:
            scores_list.append(scores_list[-1])
            strategy_scores = self._strategy_tracker.update_score(program, -float('inf'))
            global failed_count
            failed_count += 1

        return results

    def _compile_and_run_function(self, program, function_to_run, function_to_evolve, dataset, numba_accelerate,
                                  result_queue):
        try:
            # optimize the code (decorate function_to_run with @numba.jit())
            if numba_accelerate:
                program = evaluator_accelerate.add_numba_decorator(
                    program=program,
                    function_to_evolve=function_to_evolve
                )
            # compile the program, and maps the global func/var/class name to its address
            all_globals_namespace = {}
            # execute the program, map func/var/class to global namespace
            exec(program, all_globals_namespace)
            # get the pointer of 'function_to_run'
            function_to_run = all_globals_namespace[function_to_run]
            # return the execution results
            results = function_to_run(dataset)
            # the results must be int or float
            if not isinstance(results, (int, float)):
                result_queue.put((None, False))
                return
            result_queue.put((results, True))
        except  Exception as e:     #* 源代码没打印错误信息...
            print("Sandox error:", e)        #* 打印出来一看... 艹, 居然是没装 numba 库... 装完后顺利解决
            # if raise any exception, we assume the execution failed
            result_queue.put((None, False))

if __name__ == '__main__':
    with open("bin_packing/spec.py", "r", encoding="utf-8") as f:
        specification = f.read()

    class_config = config.ClassConfig(llm_class=LLMAPI, sandbox_class=Sandbox)

    config = config.Config(samples_per_prompt=4, evaluate_timeout_seconds=300)

    bin_packing_or3 = {'OR3': bin_packing_utils.datasets['OR3']}
    global_max_sample_num = 50 * 4  # n * m, n is total number of rounds and m is the number of samplers
    
    print("\nStarting FunSearch with strategy tracking...")
    # 开始时间
    start_time = time.time()
    funsearch.main(
        specification=specification,
        inputs=bin_packing_or3,
        config=config,
        max_sample_nums=global_max_sample_num,
        class_config=class_config,
        log_dir='logs/funsearch_llm_api',
    )
    # 结束时间
    end_time = time.time()
    # 计算运行时间
    run_time = end_time - start_time

    print("\nGenerating plots...")
    # Plot overall score progression
    plt.figure(figsize=(15, 6))
    
    # Left subplot: Overall score
    plt.subplot(1, 2, 1)
    if scores_list:
        max_score_index = scores_list.index(max(scores_list))
        plt.plot(range(len(scores_list)), scores_list, 'b-', label='Overall Score')
        plt.scatter(max_score_index, scores_list[max_score_index], color='red', 
                   label=f'Max Score ({max_score_index}, {scores_list[max_score_index]:.2f})')
    plt.title('Overall Score Progression')
    plt.xlabel('Sample Number')
    plt.ylabel('Score')
    plt.legend()
    plt.grid(True)

    # Right subplot: Strategy scores
    plt.subplot(1, 2, 2)

    if not strategy_scores:
        print("Warning: No strategies were recorded!")
    else:
        # Prepare data for the pie chart
        strategies = list(strategy_scores.keys())
        max_scores = [max(scores) if scores else -np.inf for scores in strategy_scores.values()]
        samples = [len(scores) for scores in strategy_scores.values()]
        total_score = sum(samples)

        # 根据samples占比绘制饼图，在图例中标注每种策略的最高分
        wedges, texts, autotexts = plt.pie(
            samples,
            labels=strategies,
            autopct=lambda p: f'{p:.1f}%',
            startangle=140,
        )
        for i, a in enumerate(autotexts):
            a.set_text(f'{strategies[i]}: {max_scores[i]:.2f}, {samples[i]}')
            a.set_color('black')
            a.set_fontsize(10)
        plt.setp(texts, size=10)
        plt.setp(autotexts, size=10)
        plt.axis('equal')
        plt.title('Strategy Score Distribution (Best Score, Sample Count)')
        plt.legend()

    plt.tight_layout()
    # 保存图片
    plt.savefig('strategy_scores.png')
    plt.show()

    # Print final strategy statistics
    print("\nFinal Strategy Statistics:")
    print("=" * 50)
    for strategy in strategy_scores:
        scores = strategy_scores[strategy]

        if scores:
            print(f"{strategy}:")
            print(f"  Best Score: {max(scores):.2f}")
            print(f"  Best Attempt: {scores.index(max(scores))}")
            print(f"  Average Score: {sum(scores) / len(scores):.2f}")
            print(f"  Number of Total Attempts: {len(scores)}")
            if strategy in fixed_count:
                print(f"  Number of Attempts with Fixed Strategy: {fixed_count[strategy]}")  # 打印固定策略的次数
            print("-" * 50)

    print("=" * 50)
    print("Summary:\n")
    print(f"Total Samples: {len(scores_list)}")
    print(f"Best Overall Score: {max(scores_list):.2f}")
    print(f"Best Overall Attempt: {scores_list.index(max(scores_list))}")
    print(f"Total Failed Attempts: {failed_count} ({failed_count / len(scores_list) * 100:.2f}%)")
    print(f"Total Time: {run_time:.2f} seconds ({run_time / len(scores_list):.2f} seconds per attempt on average)")
