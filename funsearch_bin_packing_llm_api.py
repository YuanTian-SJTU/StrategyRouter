import json
import multiprocessing
from typing import Collection, Any
from matplotlib import pyplot as plt
import http.client
from implementation import funsearch
from implementation import config
from implementation import sampler
from implementation import evaluator_accelerate
from implementation import evaluator
from implementation import code_manipulation
from implementation import strategy_tracker
import bin_packing_utils

import json
import multiprocessing
from typing import Collection, Any
import http.client
from implementation import sampler

scores_list = []
strategy_scores = {}

def _trim_preface_of_body(sample: str) -> str:
    """Trim the redundant descriptions/symbols/'def' declaration before the function body.
    Please see my comments in sampler.LLM (in sampler.py).
    Since the LLM used in this file is not a pure code completion LLM, this trim function is required.

    -Example sample (function & description generated by LLM):
    -------------------------------------
    This is the optimized function ...
    def priority_v2(...) -> ...:
        return ...
    This function aims to ...
    -------------------------------------
    -This function removes the description above the function's signature, and the function's signature.
    -The indent of the code is preserved.
    -Return of this function:
    -------------------------------------
        return ...
    This function aims to ...
    -------------------------------------
    """
    lines = sample.splitlines()
    func_body_lineno = 0
    find_def_declaration = False
    for lineno, line in enumerate(lines):
        # find the first 'def' statement in the given code
        if line[:3] == 'def':
            func_body_lineno = lineno
            find_def_declaration = True
            break
    if find_def_declaration:
        code = ''
        for line in lines[func_body_lineno + 1:]:
            code += line + '\n'
        return code
    return sample


class LLMAPI(sampler.LLM):
    """Language model that predicts continuation of provided source code.
    """

    def __init__(self, samples_per_prompt: int, trim=True):
        super().__init__(samples_per_prompt)
        self._trim = trim

    def draw_samples(self, prompt: str) -> Collection[str]:
        """Returns multiple predicted continuations of `prompt`."""
        return [self._draw_sample(prompt) for _ in range(self._samples_per_prompt)]

    def _draw_sample(self, content: str) -> str:
        # 提取策略分
        global strategy_scores
        strategy_prompt = ''
        for strategy, scores in strategy_scores.items():
            if scores:
                score = max(scores)
                strategy_prompt += f"{strategy}: Best score {score:.2f}\n"
            else:
                strategy_prompt += (f"{strategy}: Unknown"
                                    f"\n")

        additional_prompt = (
            'Complete a different and more complex Python function. '
            'Be creative and you can implement various strategies like First Fit, Best Fit, Worst Fit, or Greedy approaches. '
            'You can also combine multiple strategies or create new ones. '
            'Only output the Python code, no descriptions.'
            'In the function docstring, clearly state which strategy you are using.'
            f'Current strategy scores:\n {strategy_prompt}'
        )
        prompt = '\n'.join([content, additional_prompt])
        
        while True:
            # 连接API信息
            try:
                conn = http.client.HTTPSConnection("api.siliconflow.cn")
                payload = json.dumps({
                    "max_tokens": 512,
                    "model": "THUDM/GLM-4-32B-0414",
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ]
                })
                headers = {
                    'Authorization': 'Bearer sk-kotbrhuawsgwcwaoiabssxsfvwvthkexamtyhljlfuwesybw',
                    'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',
                    'Content-Type': 'application/json'
                }
                conn.request("POST", "/v1/chat/completions", payload, headers)
                res = conn.getresponse()
                data = res.read().decode("utf-8")
                data = json.loads(data)
                response = data['choices'][0]['message']['content']
                # trim function
                if self._trim:
                    response = _trim_preface_of_body(response)
                return response
            except Exception:
                continue


class Sandbox(evaluator.Sandbox):
    """Sandbox for executing generated code. Implemented by RZ.

    RZ: Sandbox returns the 'score' of the program and:
    1) avoids the generated code to be harmful (accessing the internet, take up too much RAM).
    2) stops the execution of the code in time (avoid endless loop).
    """

    def __init__(self, verbose=False, numba_accelerate=True):
        """
        Args:
            verbose         : Print evaluate information.
            numba_accelerate: Use numba to accelerate the evmport abstractmethod, ABCaluation. It should be noted that not all numpy functions
                              support numba acceleration, such as np.piecewise().
        """
        self._verbose = verbose
        self._numba_accelerate = numba_accelerate
        self._strategy_tracker = strategy_tracker.StrategyTracker()
    
    def run(
            self,
            program: str,
            function_to_run: str,
            function_to_evolve: str,
            inputs: Any,
            test_input: str,
            timeout_seconds: int,
            **kwargs
    ) -> tuple[Any, bool]:
        """Returns `function_to_run(test_input)` and whether execution succeeded."""
        dataset = inputs[test_input]
        result_queue = multiprocessing.Queue()
        process = multiprocessing.Process(
            target=self._compile_and_run_function,
            args=(program, function_to_run, function_to_evolve, dataset, self._numba_accelerate, result_queue)
        )
        process.start()
        process.join(timeout=timeout_seconds)
        if process.is_alive():
            # if the process is not finished in time, we consider the program illegal
            process.terminate()
            process.join()
            results = None, False
        else:
            if not result_queue.empty():
                results = result_queue.get_nowait()
            else:
                results = None, False

        if self._verbose:
            print(f'================= Evaluated Program =================')
            program_: code_manipulation.Program = code_manipulation.text_to_program(text=program)
            func_to_evolve_: str = kwargs.get('func_to_evolve', 'priority')
            function_: code_manipulation.Function = program_.get_function(func_to_evolve_)
            function_: str = str(function_).strip('\n')
            print(f'{function_}')
            print(f'-----------------------------------------------------')
            print(f'Score: {str(results)}')
            print(f'=====================================================')
            print(f'\n')
        
        # 记录当前最高分和策略分数
        global strategy_scores  # 显式声明为全局变量
        if results[0] is not None:  # 如果分数不为空
            # 分数列表
            if not scores_list: # 如果分数列表为空，直接添加当前分数
                scores_list.append(results[0])
            else:   # 如果分数列表为空，添加当前分数和列表中上一分数更高的一个
                scores_list.append(max(scores_list[-1], results[0]))

            # 策略分数
            strategy_scores = self._strategy_tracker.update_score(program, results[0])
        else:
            scores_list.append(scores_list[-1])
            strategy_scores = self._strategy_tracker.update_score(program, -float('inf'))

        return results

    def _compile_and_run_function(self, program, function_to_run, function_to_evolve, dataset, numba_accelerate,
                                  result_queue):
        try:
            # optimize the code (decorate function_to_run with @numba.jit())
            if numba_accelerate:
                program = evaluator_accelerate.add_numba_decorator(
                    program=program,
                    function_to_evolve=function_to_evolve
                )
            # compile the program, and maps the global func/var/class name to its address
            all_globals_namespace = {}
            # execute the program, map func/var/class to global namespace
            exec(program, all_globals_namespace)
            # get the pointer of 'function_to_run'
            function_to_run = all_globals_namespace[function_to_run]
            # return the execution results
            results = function_to_run(dataset)
            # the results must be int or float
            if not isinstance(results, (int, float)):
                result_queue.put((None, False))
                return
            result_queue.put((results, True))
        except  Exception as e:     #* 源代码没打印错误信息...
            print("Sandox error:", e)        #* 打印出来一看... 艹, 居然是没装 numba 库... 装完后顺利解决
            # if raise any exception, we assume the execution failed
            result_queue.put((None, False))

if __name__ == '__main__':
    with open("bin_packing/spec.py", "r", encoding="utf-8") as f:
        specification = f.read()

    class_config = config.ClassConfig(llm_class=LLMAPI, sandbox_class=Sandbox)

    config = config.Config(samples_per_prompt=4, evaluate_timeout_seconds=300)

    bin_packing_or3 = {'OR3': bin_packing_utils.datasets['OR3']}
    global_max_sample_num = 3 * 4  # n * m, n is total number of rounds and m is the number of samplers
    
    print("\nStarting FunSearch with strategy tracking...")
    funsearch.main(
        specification=specification,
        inputs=bin_packing_or3,
        config=config,
        max_sample_nums=global_max_sample_num,
        class_config=class_config,
        log_dir='logs/funsearch_llm_api',
    )

    print("\nGenerating plots...")
    # Plot overall score progression
    plt.figure(figsize=(15, 6))
    
    # Left subplot: Overall score
    plt.subplot(1, 2, 1)
    if scores_list:
        max_score_index = scores_list.index(max(scores_list))
        plt.plot(range(len(scores_list)), scores_list, 'b-', label='Overall Score')
        plt.scatter(max_score_index, scores_list[max_score_index], color='red', 
                   label=f'Max Score ({max_score_index}, {scores_list[max_score_index]:.2f})')
    plt.title('Overall Score Progression')
    plt.xlabel('Sample Number')
    plt.ylabel('Score')
    plt.legend()
    plt.grid(True)

    # Right subplot: Strategy scores
    plt.subplot(1, 2, 2)
    
    if not strategy_scores:
        print("Warning: No strategies were recorded!")
    
    # Define colors for different strategies
    colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']
    for i, strategy in enumerate(strategy_scores):
        color = colors[i % len(colors)]
        scores = strategy_scores[strategy]
        plt.plot(range(len(scores)), scores, f'{color}-', label=strategy)
    
    plt.title('Strategy Score Progression')
    plt.xlabel('Sample')
    plt.ylabel('Score')
    plt.legend()
    
    plt.tight_layout()
    plt.show()

    # Print final strategy statistics
    print("\nFinal Strategy Statistics:")
    print("=" * 50)
    for strategy in strategy_scores:
        scores = strategy_scores[strategy]
        if scores:
            print(f"{strategy}:")
            print(f"  Best Score: {max(scores):.2f}")
            print(f"  Average Score: {sum(scores)/len(scores):.2f}")
            print(f"  Number of Attempts: {len(scores)}")
            print("-" * 50)
